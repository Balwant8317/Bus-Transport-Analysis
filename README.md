* **Programming Languages:** Python,SQL,Pyspark
* **Big Data Technologies:** Hadoop (HDFS, MapReduce), Apache Hive, Apache Spark (Spark SQL, Spark Streaming)
* **Cloud Platforms:** AWS, GCP  Azure*
* **Machine Learning:** scikit-learn, TensorFlow
* **Data Visualization:** Power BI, Amazon QuickSight,Looker Studio,Azure Power BI, Tableau
* **Data Storage:** Azure Blob, AWS S3, CGP

## Data Source

## Project Methodology

1. **Data Collection and Preprocessing:** Data Is Provided By Our Univesity by kaggle
2. **Exploratory Data Analysis (EDA):** I Have Done EDA Using Pyspark 
3. **Predictive Model Development:** Done by using Machine Learning Alorithms ex Liner Regression
4. **Data Visualization:** PowerBI,Amazon QuickSight,Looker Studio,Azure Power BI,

## Azure

1. Data Storage: Azure Blob Storage

Instead of: Storing data in local CSV files or on HDFS.
Used: Azure Blob Storage. It's scalable, cost-effective, and integrates well with other Azure services.

2. Data Processing: Azure Databricks (Spark)

Instead of: Running Spark locally or on a Hadoop cluster.
Used: Azure Databricks. It provides a managed Spark environment in the cloud.

3. Model Training: Azure Machine Learning

Instead of: Training models locally.
Used: Azure Machine Learning. It offers a platform for training, deploying, and managing machine learning models.
4. Data Visualization: Azure Power BI

Instead of: Just using local Power BI Desktop.

Used: Azure Power BI.  Publishing Our dashboards to the cloud for sharing and collaboration.

Connected Power BI Desktop to our data in Blob Storage and Design your dashboards and reports.

Published our reports to the Power BI service in Azure.

## GCP (Google Cloud Platform)
1. Data Storage: Google Cloud Storage (GCS)

Instead of: Local CSV files or HDFS.
Used: Google Cloud Storage (GCS). It's scalable, cost-effective, and integrates well with other GCP services.

2. Data Processing: Google Cloud Dataproc (Spark)

Instead of: Running Spark locally or on a Hadoop cluster.
Used: Google Cloud Dataproc. It provides a managed Spark and Hadoop environment.

3. Model Training: Vertex AI

Instead of: Training models locally.
Used: Vertex AI. It's GCP's platform for training, deploying, and managing machine learning models.

4. Data Visualization: Looker Studio

Instead of: Just using local Power BI Desktop.

Used: Looker Studio. It's GCP's data visualization and reporting tool.

Connect Looker Studio to our data in GCS or BigQuery (GCP's data warehouse).

Designe our dashboards and reports

## AWS
1. Data Storage: Amazon S3

Instead of: Local CSV files or HDFS.
Used: Amazon S3 (Simple Storage Service). It's highly scalable, durable, and integrates seamlessly with other AWS services.

2. Data Processing: Amazon EMR (with Spark)

Instead of: Running Spark locally or on a Hadoop cluster.
Used: Amazon EMR (Elastic MapReduce). It provides a managed Hadoop and Spark environment in the cloud.

3. Model Training: Amazon SageMaker

Instead of: Training models locally.
Used: Amazon SageMaker. It's AWS's platform for building, training, and deploying machine learning models.

4. Data Visualization: Amazon QuickSight

Instead of: Just using local Power BI Desktop.

Used: Amazon QuickSight. It's AWS's cloud-based business intelligence service.

Connected QuickSight to our data in S3, Redshift (AWS's data warehouse)

for Creating visualizations, dashboards, and reports.



